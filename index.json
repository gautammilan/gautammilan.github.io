[{"authors":null,"categories":null,"content":"I am from Nepal, where I have completed my bachelor’s in computer engineering from Tribhuvan University, Institute of Engineering, Paschimanchal Campus.\nCurrently, I am working at Palua.Ai Ltd as an AI engineer, where I developed the first ever state-of-the-art Nepali language model trained on a large dataset along with the Nep-gLUE benchmark. I am now working on active learning research for both computer vision and NLP problems to make the data annotation process faster, increase label efficency, and reduce the cost of annotation.\n","date":1663632e3,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1663632e3,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am from Nepal, where I have completed my bachelor’s in computer engineering from Tribhuvan University, Institute of Engineering, Paschimanchal Campus.\nCurrently, I am working at Palua.Ai Ltd as an AI engineer, where I developed the first ever state-of-the-art Nepali language model trained on a large dataset along with the Nep-gLUE benchmark.","tags":null,"title":"Milan Gautam","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://gautammilan.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Milan Gautam","Sulav Timilsina","Binod Bhattarai"],"categories":null,"content":" ","date":1663632e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663632e3,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://gautammilan.github.io/publication/example/","publishdate":"2022-09-20T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"NepBERTa: Nepali Language Model Trained in a Large Corpus","type":"publication"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":"With the introduction of BERT in 2018, the machine became as capable of understanding natural language as an ordinary human being, making it feasible to sift through massive corpora of text data and extract meaningful information from it. Due to high data requirements and computational resources, only a few languages have made this transition into the modern era, while many languages, such as Nepali, are still in the prehistoric days of NLP.\nAfter learning about it, it became my goal to design a Nepali language model and make it open source so that anyone could use it. I began working on the dream by gathering 14.5 GB of Nepali Corpus from over 50 Nepali news websites. To assess the performance of various input length language models, we trained two models, one with 128 token lengths and the other with 512 token lengths, using cloud TPUs. To test our model, we are now developing a GLUE-like evaluation task for the Nepali language. Because this is an active project, I may not be able to release the code.\n","date":1648914834,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648914834,"objectID":"a508d2bced41b14f292790f7346d4f0f","permalink":"https://gautammilan.github.io/project/nepali-language-model/","publishdate":"2022-04-02T21:38:54+05:45","relpermalink":"/project/nepali-language-model/","section":"project","summary":"With the introduction of BERT in 2018, the machine became as capable of understanding natural language as an ordinary human being, making it feasible to sift through massive corpora of text data and extract meaningful information from it.","tags":["Deep Learning"],"title":"Nepali Langauge Model","type":"project"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":"Speaker Identification is the process of identifying a person based on the audio of their spoken words. Previously, audio processing models were widely accepted, but convolution neural networks have lately demonstrated that they, too, may generate astounding outcomes. I’ve always wanted to be a model trainer in my mother language. As a result, I collected Nepali audio from YouTube of exactly 34 politicians, both male and female, speaking in diverse circumstances, taking care not to include noise in the audio, and the average audio length is approximately 5 minutes.\nIn this study, a siamese network with contractive loss was implemented, yielding high-quality results.\nGithub Code\nI have written a medium article in which I go into great detail about the procedures I took to solve this problem:\nArticle\n","date":1643903634,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643903634,"objectID":"228dbe1398eab3edc866ccc5c36fbb73","permalink":"https://gautammilan.github.io/project/nepali-speaker-recognition/","publishdate":"2022-02-03T21:38:54+05:45","relpermalink":"/project/nepali-speaker-recognition/","section":"project","summary":"Speaker Identification is the process of identifying a person based on the audio of their spoken words. Previously, audio processing models were widely accepted, but convolution neural networks have lately demonstrated that they, too, may generate astounding outcomes.","tags":["Deep Learning"],"title":"Nepali Speaker Recognition","type":"project"},{"authors":["Milan Gautam","Sulav Timilsina"],"categories":["Deep Learning"],"content":"GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.\nSISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model’s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it’s quite difficult to master the output up to original images in terms of richness and quality.\nData Preprocessing and Augmentation: We have used the DIV2K [Agustsson and Timofte (2017)] dataset provided by the TensorFlow library. There are altogether 800 pairs of low resolution and high-resolution images in the the training set whereas 100 pairs in the testing set. This data contains mainly people, cities, fauna, sceneries, etc. We used flipping and rotating through 90, 180, and 270 degrees randomly over the dataset. Since we had limited memory on the training computer, we had to split large images into patches of smaller size. 19 patches of size 96 ✕ 96 pixels resolution were obtained from an image randomly. Our generator is designed to upsample images by 4 times so, the output image patch will be of dimension: 384 ✕ 384 pixels.\nGenerator The generator is the block in the architecture which is responsible for generating the high resolution(HR) images from low resolution(LR) images. In 2015, SRGAN was published which introduced the concept of using GAN for SISR tasks which produced the state the art solution. The generator of SRGAN consists of several residual blocks that facilitate the flow of the gradient during backpropagation.\nTo further enhance the quality of generator images ESRGAN was released which performed some modifications in the generator of the SRGAN which includes:\nRemoving the batch normalized(BN) layers. Replacing the original residual block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as in the figure below: Fig: Residual in Residual Dense Block(RRDB)\nRemoving BN layers has proven to increase performance and reduce the computational complexity in different PSNR-oriented tasks including SR and deblurring. BN layers normalize the features using mean and variance in a batch during training and use the estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. The researchers empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the need for stable performance overtraining. Therefore, removing BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.\nRRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in the figure above, the proposed RRDB has a residual-in-residual structure, where residual learning is used at different levels. Here, the RRDB uses dense block in the main path, where the network capacity becomes higher benefiting from the dense connections. In addition to the improved architecture, it also exploits several techniques to facilitate training a very deep network such as residual scaling(beta) i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability.\nDiscriminator The task of the discriminator is to discriminate between real HR images and generated SR images. Discriminator architecture here used is similar to DC-GAN architecture with LeakyReLU activation function. The network contains eight convolutional layers with 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. But to overcome the …","date":1643817234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643817234,"objectID":"ae4895c84a8807dbe3c38da7af8b3ae1","permalink":"https://gautammilan.github.io/post/single-image-super-resolution/","publishdate":"2022-02-02T21:38:54+05:45","relpermalink":"/post/single-image-super-resolution/","section":"post","summary":"GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary.","tags":[],"title":"Single Image Super Resolution","type":"post"},{"authors":["Milan Gautam","Sulav Timilsina"],"categories":["Deep Learning"],"content":"SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model’s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it’s quite difficult to master the output up to original images in terms of richness and quality.\nIn this study, we used Wasserstein GAN with Gradient Penalty to train SRGAN, ensuring steady training of both generator and discriminator.\nGithub Code\nPlease visit this article where I have explained each and every single thing that are crucial from data processing to training.\nArticle\n","date":1643817234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643817234,"objectID":"68cb71b8a31acd5ef1ce3b6673d64d51","permalink":"https://gautammilan.github.io/project/1single-image-super-resolution/","publishdate":"2022-02-02T21:38:54+05:45","relpermalink":"/project/1single-image-super-resolution/","section":"project","summary":"SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information.","tags":["Deep Learning"],"title":"Single Image Super Resolution","type":"project"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":" In statistical terms, time series forecasting is the process of analyzing the time series data using statistics and modeling to make predictions and informed strategic decisions. It falls under Quantitative Forecasting. Examples of Time Series Forecasting are weather forecast over next week, forecasting the closing price of a stock each day etc. In this article we will see different types of models which can be used for this analysis and pick what is best for what situations.\nTime series data Time series data are simply measurements or events that are tracked, monitored, downsampled, and aggregated over time. This could be server metrics, application performance monitoring, network data, sensor data, events, clicks, trades in a market, and many other types of analytics data. We will be taking stock price data to perform our analysis.\nNabil bank is a bank located in Nepal, it’s been trading in NEPSE(Nepal Stock Exchange ) for the past 20 years. We can easily get this data by going to the NEPSE website. This data contains four features such as the price of the stock when the market opens on a particular date, the maximum value of the stock, the minimum value, and the value of the stock at which the market close on that day.\nDataset Preprocessing 1. Normalization We need to normalize between 0-1, to remove the problem which arises if the features having in different scales. But when normalizing validate and test data don’t use the validate.max() or text.max() and validate.min() or text.min() for their respective normalization use train.max and train.min for both of them. Because we can’t look at the validate or test dataset they are unknown to us. The important thing to note here is that the normalization has been done on the input feature only not on the label, the model will predict the actual value of stock.\n2. Sliding window To perform Supervised learning the dataset should have inputs and its corresponding label. Data windowing is a popular technique for converting historical data like time series to data suitable for supervised learning. It works as it sounds, we select a window for inputs and feed the model the data which has been selected into that window and the model will try to predict the label for that window. The main features of the input windows are:\n•\tThe width (number of time steps) of the input and label windows.\n•\tThe time offset between them.\n•\tWhich features are used as inputs, labels, or both.\nDepending on the task and type of model we may want to generate a variety of data windows. Here are some examples:\nA model that makes a prediction one hour into the future given six days of history, would need a window like this: Example 1 Similarly, to make a single prediction 24 days into the future, given 24 days of history, we might define a window like this: Example 1 source\nTherefore, depending upon the task and model we can generate varieties of inputs which helps to reduce the redundancy of code as by defining an data window using a class.\nModels In time series forecasting depending upon the number of steps we are going to do the prediction for the models can be classified into two types:\n1. Single step Model In single step model, model will look one step into the future. For example given all the past one month of stock data model will predict what will be the stock value tomorrow. For this task we will be using models like:\n1.1\tDense model: A single dense layer is a single layer of fully connected neural network. Here, we will be sending our Inputs of specific input width into multiple dense layer and finally the output of these dense layer will be send though a single neuron dense layer to produce a single step output. It is an regression problem where we take Open, Close, Low and High as input to predict the closing value of the stock.\ndef dense_func(input_shape): input= tf.keras.Input(shape= tf.constant(input_shape)) x= tf.keras.layers.Flatten()(input) #Basically there are four dense layer each followed by an dropout layer x= tf.keras.layers.Dense(units=556, activation=\u0026#39;relu\u0026#39;)(x) x= tf.keras.layers.Dropout(0.2)(x) x= tf.keras.layers.Dense(units=228, activation=\u0026#39;relu\u0026#39;)(x) x= tf.keras.layers.Dropout(0.2)(x) x= tf.keras.layers.Dense(units=128, activation=\u0026#39;relu\u0026#39;)(x) x= tf.keras.layers.Dropout(0.2)(x) x= tf.keras.layers.Dense(units=64, activation=\u0026#39;relu\u0026#39;)(x) x= tf.keras.layers.Dropout(0.2)(x) output= tf.keras.layers.Dense(units=1)(x) model= tf.keras.Model(inputs= input,outputs= output) return model Architecture a. Hyperparameter tuning One of the most important hyperparameter for stock price prediction is the number of days that the model sees to make future prediction ie input_width. This hyperparameter value is calculated by training the model on different number of input width and the model which produces lowest loss it is selected.\nInput width vs Mean square Error(MSE) We trained the model on input width [3,5,8,15,18,22,25,28,31] and among them the minimum value of MSE was obtained with …","date":1643817234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643817234,"objectID":"3fc72d77d3de20c5a20f581b609d7f6b","permalink":"https://gautammilan.github.io/post/stock-price-analysis/","publishdate":"2022-02-02T21:38:54+05:45","relpermalink":"/post/stock-price-analysis/","section":"post","summary":"In statistical terms, time series forecasting is the process of analyzing the time series data using statistics and modeling to make predictions and informed strategic decisions. It falls under Quantitative Forecasting.","tags":[],"title":"Stock Price forecasting On Nabil Bank","type":"post"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":" Nabil bank is a bank located in Nepal, it’s been trading in NEPSE(Nepal Stock Exchange ) for the past 20 years. We can easily get this data by going to the NEPSE website. This data contains four features such as the price of the stock when the market opens on a particular date, the maximum value of the stock, the minimum value, and the value of the stock at which the market close on that day.\nDescription There are two types of models that are widely used for time series analysis they are:\nSingle Step model= Here model will look one step into the future. For example, given all the past one month of the stock data model will predict what will be the stock value tomorrow. The dense model and LSTM model are used to evaluate the single-step model.\nMulti-step model= In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single-step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values. The autoregressive model is used for this task.\nTherefore in this project, we analyzed different models and evaluate which works best for our data.\nGithub Code\nPlease visit this article below where every single step is explained thoroughly.\nArticle\n","date":1643817234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643817234,"objectID":"795ec18f182f4dd678f08b74e121dcbf","permalink":"https://gautammilan.github.io/project/stock-price-analysis/","publishdate":"2022-02-02T21:38:54+05:45","relpermalink":"/project/stock-price-analysis/","section":"project","summary":"Nabil bank is a bank located in Nepal, it’s been trading in NEPSE(Nepal Stock Exchange ) for the past 20 years. We can easily get this data by going to the NEPSE website.","tags":["Deep Learning"],"title":"Stock Price forecasting On Nabil Bank","type":"project"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":" As most of today’s world has been digitalized, instead of writing transactions in journals people do it on their computers. In many cases, these transactions have important information about the parties involved in it, so business tries to acquire it by using many techniques. In this article, we will look into one such transaction and try to acquire embedded information inside the transaction using the deep learning model.\nConventional Method As we can see the transaction contains the store number information, usually people had to go through all of these transactions to get the store number ID. But in today’s era, it’s not feasible where thousands of transaction happens every single second using various digital means.\nMapping it to machine learning problem So basically we need to build a model which will make the transaction as an input and produce its store number. As we are emphasizing each character and determining whether it is a store number or not. Character LSTM is capable of doing this task, so the ideal choice for solving this problem is a char-LSTM model, so we will be using it.\nWorking of Bidirectional LSTM We will approach this problem as binary classification where the position of store numbers present inside the transaction is labeled as 1 and all the other characters as 0. During inference, we will select all the characters as store numbers that have the prediction of 1. The position of the store number is not rigid and it depends upon the characters appearing on both sides of it. So it is important to look at the transaction from left to right and right to left and then select the store number. Therefore bidirectional LSTM is capable of doing that and we will be using it in this project. Similarly, the size of data that we will be working on is really small only 100 data points for train and we will use other 100 data points for validation, so we will use different training approaches to get the best out of it.\nDataset Preprocessing 1. 1.\tRemoving consecutive Whitespaces We are considering whitespace as a special character, if multiple whitespaces appear in the transaction then they get tokenized using consecutive whitespace tokens, here the model will learn the necessity of adding consecutive whitespaces which is not true. So by removing the consecutive whitespaces altogether we elimate these phenomena. For example\nBefore removal: Spark 2001\nAfter removal: Spark 2001\n1.2 Removing names All the transactions have some kind of name inside them like ANN TAYLOR FACTORY #2202, MCDONALD’S F1682, etc. For some models, these names accelerate the training and for some, it works counterproductive. Depending upon the model we will be removing names from the transaction.\nBefore removal: Spark 2001\nAfter removal: 2001\n1.3 Tokenization amd Padding Dictionary is created for all the characters(a,A,c,C…), non-characters(@,#,%,…) and digits(1,2,3,4,..). Two special tokens for whitespace character and padding([PAD]) are also included in the dictionary which makes the total size of the vocabulary 87. Finally, all the characters of the transactions are tokenized using the dictionary.\nWe have considered that every transaction will have 50 characters, to make sure every transaction has the same length. If the transaction is smaller than this it is padded using padding character [PAD] and if it is larger than 50 characters then it is truncated.\n1.4 Label Creating A list of labels is created for each transaction. If the character is a store number character then it is labeled as 1 elsewhere it is labeled as 0. For transactions such as DOLRTREE 2257 00022574, where the characters forming the store number are located at two places. In such a case the store number characters which are located on the leftmost side are labeled as store number characters and the other one is ignored.\nTransaction: DOLRTREE 2257 00022574\nlabel :00000000 1111 00000000\nNote: For this example whitespace characters has not be labeled.\nModels 1.\tRegular expression model By addressing different scenario that occur on training dataset different regular expression operation has been performed on the data. The accuracy for this model is:\na.\tTraining dataset= 97%\nb.\tValidation dataset= 87%\nJust by using regular expression the accuracy of validation data is also really good which indicates there is not much difference between the training and validation set. As the possibility of overfitting is really small, we can even train the model for a large number of epochs.\n2.\tLSTM Model There are two inputs to the model one is the token and the second is the attention mask which is a Boolean matrix indicating which tokens have been padded, it is useful when training and calculating loss so that the loss of the padded token doesn’t get included on our overall loss. Similarly, as the store-number character is dependent on the characters on both sides we will use bidirectional LSTM. Bidirectional LSTM produces two outputs for every single cell, one when …","date":1641225234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641225234,"objectID":"ddd9e93cdb794de46453e668fa10a15f","permalink":"https://gautammilan.github.io/post/entity-extraction/","publishdate":"2022-01-03T21:38:54+05:45","relpermalink":"/post/entity-extraction/","section":"post","summary":"As most of today’s world has been digitalized, instead of writing transactions in journals people do it on their computers. In many cases, these transactions have important information about the parties involved in it, so business tries to acquire it by using many techniques.","tags":[],"title":"Transactions entity extractor","type":"post"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":" Because most of today’s world has been digitalized, instead of recording transactions in journals, people record them on their computers. In many circumstances, these transactions contain critical information about the parties involved, thus businesses attempt to obtain it through a variety of means. In this study, we will examine one such transaction and attempt to acquire embedded information within it using a deep learning model.\nConventional Method So, in this study, I created a model that takes a transaction as an input and generates its store number. While highlighting each character and evaluating whether or not it is a store number. Because character LSTM is capable of doing this task, a char-LSTM model is the best solution for handling this problem, and we will use it.\nGithub Code\nThe following article details the steps I took to address this issue:\nArticle\n","date":1641225234,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641225234,"objectID":"1e774122ced314feb8f06fca43189960","permalink":"https://gautammilan.github.io/project/entity-extraction/","publishdate":"2022-01-03T21:38:54+05:45","relpermalink":"/project/entity-extraction/","section":"project","summary":"Because most of today’s world has been digitalized, instead of recording transactions in journals, people record them on their computers. In many circumstances, these transactions contain critical information about the parties involved, thus businesses attempt to obtain it through a variety of means.","tags":["Deep Learning"],"title":"Transactions entity extractor","type":"project"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":" I have written an medium article examplaning the things that one need to consider when trying to train on your own messages. Please visit here: Code\n","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606694400,"objectID":"dd11d35415ae182604ab7c6afe3ec806","permalink":"https://gautammilan.github.io/post/fb-bot/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/post/fb-bot/","section":"post","summary":"I have written an medium article examplaning the things that one need to consider when trying to train on your own messages. Please visit here: Code","tags":[],"title":"Facebook Messaging Bot","type":"post"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":" I used my Facebook conversations to train the RNN model. So the model will aim to mimic how I converse with people, only by providing the initial character for an input, say ‘h,’ will generate an entire message beginning with the character ‘h,’ such as “how are you doing today?”. Because the model was trained on a single character, it may now be trained in any language. Github Code\nThe following article details the steps I took to address this problem:\nArticle\n","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606694400,"objectID":"67ade05bc5c4014231f9c58678caba6c","permalink":"https://gautammilan.github.io/project/facebook-messaging-bot/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/project/facebook-messaging-bot/","section":"project","summary":"I used my Facebook conversations to train the RNN model. So the model will aim to mimic how I converse with people, only by providing the initial character for an input, say ‘h,’ will generate an entire message beginning with the character ‘h,’ such as “how are you doing today?","tags":["Deep Learning"],"title":"Facebook Messaging Bot","type":"project"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":"DonorsChoose is a United States-based nonprofit organization that allows individuals to donate directly to public school classroom projects. According to last year data around 500,000 proposals were sent by teachers all around the world to DonorsChoose hoping to get the donation for their respective projects. Organization goes through every single project proposal to select those proposals which have a higher probability of getting donations. After selecting the project Donor Choose organization will display the selected project on their website allowing the donor to go through the project which they are likely to donate. Here the main problem is going through all the project proposal equired a large number of resources and cost a tremendous amount of money.\nObjective To apply deep learning model in order to automize the task of selecting those proposal which have high possibility of getting donation. Model will produce an output for every single proposal, where output ‘1’ indicates that the proposal has high chance of acceptance and ‘0’ indicates the rejection probability\nDeployment I have done the deployment using streamlit, inorder to get more information please visit the website here\nhttps://share.streamlit.io/gautammilan/proposal-recommendation/main/website.py\n","date":1561161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561161600,"objectID":"97f8f25d29159b8e2c13664c96605d87","permalink":"https://gautammilan.github.io/project/donor-choose/","publishdate":"2019-06-22T00:00:00Z","relpermalink":"/project/donor-choose/","section":"project","summary":"DonorsChoose is a United States-based nonprofit organization that allows individuals to donate directly to public school classroom projects. According to last year data around 500,000 proposals were sent by teachers all around the world to DonorsChoose hoping to get the donation for their respective projects.","tags":["Deep Learning"],"title":"Proposal Recommendation and Deployment","type":"project"},{"authors":["Milan Gautam"],"categories":["Deep Learning"],"content":"After learning that an NLP task might be performed using CNN, I was fascinated. So, throughout the study, we will classify emails using CNN. The dataset includes roughly 18000 emails divided into 20 categories such as sports, health, religion, and so on. The network’s core backbone is the 1D convolution neural network, which is used for classification.\nGithub Code\n","date":1561161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561161600,"objectID":"dc98d4d36dc2b86ad20a6a52cfbeb9fc","permalink":"https://gautammilan.github.io/project/text-classification-cnn/","publishdate":"2019-06-22T00:00:00Z","relpermalink":"/project/text-classification-cnn/","section":"project","summary":"After learning that an NLP task might be performed using CNN, I was fascinated. So, throughout the study, we will classify emails using CNN. The dataset includes roughly 18000 emails divided into 20 categories such as sports, health, religion, and so on.","tags":["Deep Learning"],"title":"Text Classification using CNN ","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://gautammilan.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://gautammilan.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]