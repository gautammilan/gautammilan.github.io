<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | MILAN</title><link>https://gautammilan.github.io/project/</link><atom:link href="https://gautammilan.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 02 Apr 2022 21:38:54 +0545</lastBuildDate><image><url>https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://gautammilan.github.io/project/</link></image><item><title>Nepali Langauge Model</title><link>https://gautammilan.github.io/project/nepali-language-model/</link><pubDate>Sat, 02 Apr 2022 21:38:54 +0545</pubDate><guid>https://gautammilan.github.io/project/nepali-language-model/</guid><description>&lt;p>With the introduction of BERT in 2018, the machine became as capable of understanding natural language as an ordinary human being, making it feasible to sift through massive corpora of text data and extract meaningful information from it. Due to high data requirements and computational resources, only a few languages have made this transition into the modern era, while many languages, such as Nepali, are still in the prehistoric days of NLP.&lt;/p>
&lt;p>After learning about it, it became my goal to design a Nepali language model and make it open source so that anyone could use it. I began working on the dream by gathering 14.5 GB of Nepali Corpus from over 50 Nepali news websites. To assess the performance of various input length language models, we trained two models, one with 128 token lengths and the other with 512 token lengths, using cloud TPUs. To test our model, we are now developing a GLUE-like evaluation task for the Nepali language. Because this is an active project, I may not be able to release the code.&lt;/p></description></item><item><title>Nepali Speaker Recognition</title><link>https://gautammilan.github.io/project/nepali-speaker-recognition/</link><pubDate>Thu, 03 Feb 2022 21:38:54 +0545</pubDate><guid>https://gautammilan.github.io/project/nepali-speaker-recognition/</guid><description>&lt;p>Speaker Identification is the process of identifying a person based on the audio of their spoken words. Previously, audio processing models were widely accepted, but convolution neural networks have lately demonstrated that they, too, may generate astounding outcomes. I&amp;rsquo;ve always wanted to be a model trainer in my mother language. As a result, I collected Nepali audio from YouTube of exactly 34 politicians, both male and female, speaking in diverse circumstances, taking care not to include noise in the audio, and the average audio length is approximately 5 minutes.&lt;/p>
&lt;p>In this study, a siamese network with contractive loss was implemented, yielding high-quality results.&lt;/p>
&lt;br>
&lt;p>&lt;a href="https://github.com/gautammilan/Nepali_Speaker_Recognition" target="_blank" rel="noopener">Github Code&lt;/a>&lt;/p>
&lt;p>I have written a medium article in which I go into great detail about the procedures I took to solve this problem:&lt;/p>
&lt;p>&lt;a href="https://medium.com/@milangautam5198/identifying-nepali-politician-from-audio-c0365f74f8ef" target="_blank" rel="noopener">Article&lt;/a>&lt;/p></description></item><item><title>Single Image Super Resolution</title><link>https://gautammilan.github.io/project/1single-image-super-resolution/</link><pubDate>Wed, 02 Feb 2022 21:38:54 +0545</pubDate><guid>https://gautammilan.github.io/project/1single-image-super-resolution/</guid><description>&lt;p>SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model&amp;rsquo;s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it&amp;rsquo;s quite difficult to master the output up to original images in terms of richness and quality.&lt;/p>
&lt;p>In this study, we used Wasserstein GAN with Gradient Penalty to train SRGAN, ensuring steady training of both generator and discriminator.&lt;/p>
&lt;br>
&lt;p>&lt;a href="https://github.com/gautammilan/Single-Image-Super-Resolution" target="_blank" rel="noopener">Github Code&lt;/a>&lt;/p>
&lt;p>Please visit this article where I have explained each and every single thing that are crucial from data processing to training.&lt;/p>
&lt;p>&lt;a href="https://gautammilan.github.io/post/single-image-super-resolution/" target="_blank" rel="noopener">Article&lt;/a>&lt;/p></description></item><item><title>Stock Price forecasting On Nabil Bank</title><link>https://gautammilan.github.io/project/stock-price-analysis/</link><pubDate>Wed, 02 Feb 2022 21:38:54 +0545</pubDate><guid>https://gautammilan.github.io/project/stock-price-analysis/</guid><description>&lt;!-- ## Introduction -->
&lt;p>Nabil bank is a bank located in Nepal, it&amp;rsquo;s been trading in NEPSE(Nepal Stock Exchange ) for the past 20 years. We can easily get this data by going to the NEPSE website. This data contains four features such as the price of the stock when the market opens on a particular date, the maximum value of the stock, the minimum value, and the value of the stock at which the market close on that day.&lt;/p>
&lt;h1 id="description">Description&lt;/h1>
&lt;p>There are two types of models that are widely used for time series analysis they are:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Single Step model= Here model will look one step into the future. For example, given all the past one month of the stock data model will predict what will be the stock value tomorrow. The dense model and LSTM model are used to evaluate the single-step model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multi-step model= In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single-step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values. The autoregressive model is used for this task.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Therefore in this project, we analyzed different models and evaluate which works best for our data.&lt;/p>
&lt;p>&lt;a href="https://github.com/gautammilan/Stock-price-forecasting-Nabil-Bank" target="_blank" rel="noopener">Github Code&lt;/a>&lt;/p>
&lt;p>Please visit this article below where every single step is explained thoroughly.&lt;/p>
&lt;p>&lt;a href="https://gautammilan.github.io/post/stock-price-analysis/" target="_blank" rel="noopener">Article&lt;/a>&lt;/p></description></item><item><title>Transactions entity extractor</title><link>https://gautammilan.github.io/project/entity-extraction/</link><pubDate>Mon, 03 Jan 2022 21:38:54 +0545</pubDate><guid>https://gautammilan.github.io/project/entity-extraction/</guid><description>&lt;!-- ## Introduction -->
&lt;p>Because most of today&amp;rsquo;s world has been digitalized, instead of recording transactions in journals, people record them on their computers. In many circumstances, these transactions contain critical information about the parties involved, thus businesses attempt to obtain it through a variety of means. In this study, we will examine one such transaction and attempt to acquire embedded information within it using a deep learning model.&lt;/p>
&lt;p>
&lt;figure id="figure-conventional-method">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://gautammilan.github.io/images/dataset_image_entity_extractor.png#center" alt="Conventional Method" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Conventional Method
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>So, in this study, I created a model that takes a transaction as an input and generates its store number. While highlighting each character and evaluating whether or not it is a store number. Because character LSTM is capable of doing this task, a char-LSTM model is the best solution for handling this problem, and we will use it.&lt;/p>
&lt;p>&lt;a href="https://github.com/gautammilan/Transactions-entity-extractor" target="_blank" rel="noopener">Github Code&lt;/a>&lt;/p>
&lt;p>The following article details the steps I took to address this issue:&lt;/p>
&lt;p>&lt;a href="https://gautammilan.github.io/post/entity-extraction/" target="_blank" rel="noopener">Article&lt;/a>&lt;/p></description></item><item><title>Facebook Messaging Bot</title><link>https://gautammilan.github.io/project/facebook-messaging-bot/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0000</pubDate><guid>https://gautammilan.github.io/project/facebook-messaging-bot/</guid><description>&lt;!-- ## Introduction -->
&lt;p>I used my Facebook conversations to train the RNN model. So the model will aim to mimic how I converse with people, only by providing the initial character for an input, say &amp;lsquo;h,&amp;rsquo; will generate an entire message beginning with the character &amp;lsquo;h,&amp;rsquo; such as &amp;ldquo;how are you doing today?&amp;rdquo;. Because the model was trained on a single character, it may now be trained in any language.
&lt;br>&lt;/p>
&lt;p>&lt;a href="https://github.com/gautammilan/Char-RNN" target="_blank" rel="noopener">Github Code&lt;/a>&lt;/p>
&lt;p>The following article details the steps I took to address this problem:&lt;/p>
&lt;p>&lt;a href="https://medium.com/@milangautam5198/things-you-need-to-take-care-when-training-rnn-on-your-facebook-messages-befbe3dea175" target="_blank" rel="noopener">Article&lt;/a>&lt;/p></description></item><item><title>Proposal Recommendation and Deployment</title><link>https://gautammilan.github.io/project/donor-choose/</link><pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate><guid>https://gautammilan.github.io/project/donor-choose/</guid><description>&lt;p>DonorsChoose is a United States-based nonprofit organization that allows individuals to donate directly to public school classroom projects. According to last year data around 500,000 proposals were sent by teachers all around the world to DonorsChoose hoping to get the donation for their respective projects. Organization goes through every single project proposal to select those proposals which have a higher probability of getting donations. After selecting the project Donor Choose organization will display the selected project on their website allowing the donor to go through the project which they are likely to donate. Here the main problem is going through all the project proposal equired a large number of resources and cost a tremendous amount of money.&lt;/p>
&lt;br>
&lt;h1 id="objective">Objective&lt;/h1>
&lt;p>To apply deep learning model in order to automize the task of selecting those proposal which have high possibility of getting donation. Model will produce an output for every single proposal, where output &amp;lsquo;1&amp;rsquo; indicates that the proposal has high chance of acceptance and &amp;lsquo;0&amp;rsquo; indicates the rejection probability&lt;/p>
&lt;br>
&lt;h1 id="deployment">Deployment&lt;/h1>
&lt;p>I have done the deployment using streamlit, inorder to get more information please visit the website here&lt;/p>
&lt;p>&lt;a href="https://share.streamlit.io/gautammilan/proposal-recommendation/main/website.py" target="_blank" rel="noopener">https://share.streamlit.io/gautammilan/proposal-recommendation/main/website.py&lt;/a>&lt;/p></description></item><item><title>Text Classification using CNN</title><link>https://gautammilan.github.io/project/text-classification-cnn/</link><pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate><guid>https://gautammilan.github.io/project/text-classification-cnn/</guid><description>&lt;p>After learning that an NLP task might be performed using CNN, I was fascinated. So, throughout the study, we will classify emails using CNN. The dataset includes roughly 18000 emails divided into 20 categories such as sports, health, religion, and so on. The network&amp;rsquo;s core backbone is the 1D convolution neural network, which is used for classification.&lt;/p>
&lt;p>&lt;a href="https://github.com/gautammilan/Text-classification-using-CNN" target="_blank" rel="noopener">Github Code&lt;/a>&lt;/p></description></item></channel></rss>