<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.4.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Milan Gautam"><meta name=description content="GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary."><link rel=alternate hreflang=en-us href=https://gautammilan.github.io/post/single-image-super-resolution/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=/css/wowchemy.246129d782c938a644fe2d653d8a976f.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://gautammilan.github.io/post/single-image-super-resolution/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="MILAN"><meta property="og:url" content="https://gautammilan.github.io/post/single-image-super-resolution/"><meta property="og:title" content="Single Image Super Resolution | MILAN"><meta property="og:description" content="GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary."><meta property="og:image" content="https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-02-02T21:38:54+05:45"><meta property="article:modified_time" content="2022-02-02T21:38:54+05:45"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://gautammilan.github.io/post/single-image-super-resolution/"},"headline":"Single Image Super Resolution","datePublished":"2022-02-02T21:38:54+05:45","dateModified":"2022-02-02T21:38:54+05:45","author":{"@type":"Person","name":"Milan Gautam"},"publisher":{"@type":"Organization","name":"MILAN","logo":{"@type":"ImageObject","url":"https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"}},"description":"GAN (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary."}</script><title>Single Image Super Resolution | MILAN</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=ae4895c84a8807dbe3c38da7af8b3ae1><script src=/js/wowchemy-init.min.7e85d6cfe81caeb57cc49452f9b55e03.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>MILAN</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>MILAN</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/uploads/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Single Image Super Resolution</h1><div class=article-metadata><div><span>Milan Gautam</span>, <span>Sulav Timilsina</span></div><span class=article-date>Feb 2, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>8 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/deep-learning/>Deep Learning</a></span></div></div><div class=article-container><div class=article-style><p><a href=https://arxiv.org/abs/1406.2661 target=_blank rel=noopener>GAN</a> (Generating Adversarial Network) is about creating, like drawing but completely from scratch. It has got two models: the Generator and the Discriminator are put together into a game of adversary. Through which they learn the intricate details of the target data distribution. These two models are playing a MIN-MAX game where one tries to minimize the loss and the other tries to maximize. While doing so a global optimum is reached, where the Discriminator is no longer able to distinguish between real and generated (fake) data distribution.</p><p>SISR(Single Image Super-Resolution) is an application of GAN. Image super-resolution is the process of enlarging small photos while maintaining a high level of quality, or of restoring high-resolution images from low-resolution photographs with rich information. Here the model&rsquo;s work is to map the function from low-resolution image data to its high-resolution image. Instead of giving a random noise to the Generator, a low-resolution image is fed into it. After passing through various Convolutional Layers and Upsampling Layers, the Generator gives a high-resolution image output. Generally, there are multiple solutions to this problem, so it&rsquo;s quite difficult to master the output up to original images in terms of richness and quality.</p><br><h2 id=data-preprocessing-and-augmentation>Data Preprocessing and Augmentation:</h2><p>We have used the <a href=https://data.vision.ee.ethz.ch/cvl/DIV2K/ target=_blank rel=noopener>DIV2K</a> [Agustsson and Timofte (2017)] dataset provided by the TensorFlow library. There are altogether 800 pairs of low resolution and high-resolution images in the the training set whereas 100 pairs in the testing set. This data contains mainly people, cities, fauna, sceneries, etc.
We used flipping and rotating through 90, 180, and 270 degrees randomly over the dataset. Since we had limited memory on the training computer, we had to split large images into patches of smaller size. 19 patches of size 96 ‚úï 96 pixels resolution were obtained from an image randomly. Our generator is designed to upsample images by 4 times so, the output image patch will be of dimension: 384 ‚úï 384 pixels.</p><br><h2 id=generator>Generator</h2><p>The generator is the block in the architecture which is responsible for generating the high resolution(HR) images from low resolution(LR) images. In 2015, <a href=https://arxiv.org/abs/1609.04802 target=_blank rel=noopener>SRGAN</a> was published which introduced the concept of using GAN for SISR tasks which produced the state the art solution. The generator of <a href=https://arxiv.org/abs/1609.04802 target=_blank rel=noopener>SRGAN</a> consists of several residual blocks that facilitate the flow of the gradient during backpropagation.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/generator.png#center alt="Generator Architecture" loading=lazy data-zoomable></div></div></figure></p><p>To further enhance the quality of generator images <a href=https://arxiv.org/abs/1809.00219 target=_blank rel=noopener>ESRGAN</a> was released which performed some modifications in the generator of the <a href=https://arxiv.org/abs/1609.04802 target=_blank rel=noopener>SRGAN</a> which includes:</p><ul><li>Removing the batch normalized(BN) layers.</li><li>Replacing the original residual block with the proposed Residual-in-Residual Dense Block (RRDB), which combines multi-level residual network and dense connections as in the figure below:</li></ul><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/rrdb.png#center alt="RRDB Diagram" loading=lazy data-zoomable></div></div></figure></p><p>Fig: Residual in Residual Dense Block(RRDB)</p><p>Removing BN layers has proven to increase performance and reduce the computational complexity in different PSNR-oriented tasks including SR and deblurring. BN layers normalize the features using mean and variance in a batch during training and use the estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. The researchers empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework. These artifacts occasionally appear among iterations and different settings, violating the need for stable performance overtraining. Therefore, removing BN layers for stable training and consistent performance. Furthermore, removing BN layers helps to improve generalization ability and to reduce computational complexity and memory usage.</p><p>RRDB employs a deeper and more complex structure than the original residual block in SRGAN. Specifically, as shown in the figure above, the proposed RRDB has a residual-in-residual structure, where residual learning is used at different levels. Here, the RRDB uses dense block in the main path, where the network capacity becomes higher benefiting from the dense connections. In addition to the improved architecture, it also exploits several techniques to facilitate training a very deep network such as residual scaling(beta) i.e., scaling down the residuals by multiplying a constant between 0 and 1 before adding them to the main path to prevent instability.</p><br><h2 id=discriminator>Discriminator</h2><p>The task of the discriminator is to discriminate between real HR images and generated SR images. Discriminator architecture here used is similar to DC-GAN architecture with LeakyReLU activation function. The network contains eight convolutional layers with 3√ó3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels as in the VGG network. Strided convolutions are used to reduce the image resolution each time the number of features is doubled.
But to overcome the instability while training of original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). So the last sigmoid layer of the conventional DC-GAN discriminator is omitted. This helps in not restricting the feature maps in 0 to 1 value.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/discriminator.png#center alt="discriminator image" loading=lazy data-zoomable></div></div></figure></p><h2 id=losses>Losses:</h2><h3 id=generator-loss>Generator Loss</h3><p>The generator loss is the sum of MSE, perceptual loss +adversarial loss</p><p><em>l<sub>G</sub> = MSE+Perceptual Loss +Adversarial loss</em></p><p><em>l<sub>G</sub>= l<sub>MSE</sub>+l<sub>p</sub>+ l<sub>GA</sub></em></p><br><h3 id=mean-square-errormse>Mean Square Error(MSE)</h3><p>As the most common optimization objective for SISR, the pixelwise MSE loss is calculated as:</p><p><em>l<sub>MSE</sub> = ||G<sub>Œò</sub>(I<sub>LR</sub>) - I<sub>HR</sub>||<sub>2</sub><sup>2</sup></em>,</p><p>where the parameter of the generator is denoted by ; the generated image, namely I<sub>SR</sub>,is denoted by G<sub>Œò</sub>(I<sub>LR</sub>); and the ground truth is denoted by I<sub>HR</sub> . Although models with MSE loss favor a high PSNR value, the generated results tend to be perceptually unsatisfying with overly smooth textures. Despite the aforementioned shortcomings, this loss term is still kept because MSE has clear physical meaning and helps to maintain color stability.</p><br><h3 id=perceptual-loss>Perceptual Loss</h3><p>To compensate for the shortcomings of MSE loss and allow the loss function to better measure semantic and perceptual differences between images, we define and optimize a perceptual loss based on high-level features extracted from a pretrained network. The rationality of this loss term lies in that the pretrained network for classification originally has learned to encode the semantic and perceptual information that may be measured in the loss function. To enhance the performance of the perceptual loss, a 19-layer VGG network is used. The perceptual loss is actually the Euclidean distance between feature representations, which is defined as</p><p><em>l<sub>p</sub> = ||ùúô(G<sub>Œò</sub>(I<sub>LR</sub>)) - ùúô(I<sub>HR</sub>)||<sub>2</sub><sup>2</sup></em>,</p><p>where ùúô refers to the 19-layer VGG network. With this loss term, I<sub>SR</sub> and I<sub>HR</sub> are encouraged to have similar feature representations rather than to exactly match with each other in a pixel wise manner.</p><br><h3 id=adversarial-losses>Adversarial Losses:</h3><p>In <a href=https://arxiv.org/abs/1609.04802 target=_blank rel=noopener>SRGAN</a>, the adopted generative model is generative adversarial network (GAN) and it suffers from training instability. WGAN leverages the Wasserstein distance to produce a value function, which has better theoretical properties than the original GAN. However, WGAN requires that the discriminator must lie within the space of 1-Lipschitz through weight clipping, resulting in either vanishing or exploding gradients without careful tuning of the clipping threshold.<br>To overcome the flaw of clipping , a new approach is applied called Gradient Pelanty method. It is used to enforce the Lipschitz constraint. This way Wasserstein distance between two distributions to help decide when to stop the training but penalizes the gradient of the discriminator with respect to its input instead of weight clipping. With gradient penalty, the discriminator is encouraged to learn smoother decision boundaries.</p><br><h3 id=generator-loss-1>Generator Loss</h3><p><em>l<sub>GA</sub>=-ùîº[D(G<sub>Œò</sub>(I<sub>LR</sub>)]</em></p><br><h3 id=discriminator-loss>Discriminator Loss</h3><p><em>l<sub>DA</sub>=ùîº[D(G<sub>Œò</sub>(I<sub>LR</sub>)]-ùîº[D(I<sub>HR</sub>)] + Œªùîº(||‚ñΩ<sub>hat{I}</sub>D(hat{I})-1||<sub>2</sub>-1)<sup>2</sup></em></p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/work_flow.png#center alt="workflow diagram" loading=lazy data-zoomable></div></div></figure></p><p>Normally, the output of the classifier i.e. discriminator in this case is kept between 0-1 using a sigmoid function in the last layer, where if discriminator prediction 0 for an image then the image is SR likewise if the prediction is 1 then it is an HR image. Here the discriminator is trained using WGAN-GP approach <a href=https://sulavtimilsina.github.io/posts/wgan-gp/ target=_blank rel=noopener>(described here)</a>, hence the output is not bounded between 0-1 instead the discriminator will try to maximize the distance between the prediction of SR image and HR image and generator will try to minimize it. Let&rsquo;s look at the loss of the generator ie. I<sub>GA</sub> and the loss of discriminator I<sub>DA</sub> .</p><br><h3 id=understanding-discriminator-adversarial-loss>UNDERSTANDING DISCRIMINATOR ADVERSARIAL LOSS</h3><p>(not considering the gradient penalty term for making it easier to understand)</p><p><em>l<sub>DA</sub>=ùîº[D(G<sub>Œò</sub>(I<sub>LR</sub>)]-ùîº[D(I<sub>HR</sub>)]</em></p><p>(Note: <em>l<sub>DA</sub>= ùîº[D(I<sub>HR</sub>)]-ùîº[D(G<sub>Œò</sub>(I<sub>LR</sub>)]</em> if the loss of the discriminator is in this form than the discriminator will try to maximize this equation and the generator will try to minimize the I<sub>GA</sub>).</p><p>Considering D(G<sub>Œò</sub>(I<sub>LR</sub>))= 5 and D(I<sub>HR</sub>) = 5 initially when the discriminator doesn‚Äôt have the ability to differentiate between them.</p><p>Therefore the loss at the very beginning:
<em>l<sub>DA</sub>=5-5= 0,</em></p><p>The discriminator wants to minimize the loss l<sub>DA</sub>, hence increasing the distance between
D(G<sub>Œò</sub>(I<sub>LR</sub>))and D(I<sub>HR</sub>) . Suppose after the update of the gradient of the discriminator for the few step, the value of prediction becomes D(G<sub>Œò</sub>(I<sub>LR</sub>))=-2 and D(I<sub>HR</sub>) = 2 ,therefore discriminator is learning to know the difference between the LR image and the HR image, hence making the loss(l<sub>DA</sub>)= -4, Here the loss is minimized and the distance between the two predictions is maximized.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/understanding_disc_adv_loss.png#center alt="discriminator adverserial loss" loading=lazy data-zoomable></div></div></figure></p><br><h3 id=understanding-generator-adversarial-loss>UNDERSTANDING GENERATOR ADVERSARIAL LOSS</h3><p>Discriminator is trained for a few steps and then the update of the generator happens. Therefore the discriminator is kept a few steps ahead of the generator in terms of its learning. Let&rsquo;s consider the discriminator has been trained for the few steps and it predicted outputs are:</p><p><em>D(G<sub>Œò</sub>(I<sub>LR</sub>)) = -2</em>
<em>D(I<sub>HR</sub>) = 2</em></p><p>The loss of the generator is:</p><p><em>l<sub>GA</sub> = -ùîº[D(G<sub>Œò</sub>(I<sub>LR</sub>)]</em></p><p>Therefore,
<em>l<sub>GA</sub>= -(-2) = 2</em></p><p>Generator wants to minimize l<sub>GA</sub> , which can only we achieved by increasing the value of D(G<sub>Œò</sub>(I<sub>LR</sub>)) hence ultimately reducing the distance between D(G<sub>Œò</sub>(I<sub>LR</sub>)) and D(I<sub>HR</sub>) ,hence making the SR image and HR image identical as:</p><p><em>l<sub>GA</sub>= -(large positive value) ‚âà global minima</em></p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/understanding_gen_adv_loss.png#center alt="generator adverserial loss" loading=lazy data-zoomable></div></div></figure></p><br><h3 id=result-and-conclusion>Result and Conclusion:</h3><p>We chose Kaggle&rsquo;s kernel with Tesla P100 GPU to train the model. Since it has 20 million training parameters, training it for 500 epochs is a tedious job. Until now we have trained it only up to 100 epochs.
Following is the sample output of the 100th epoch.
The rightmost image is Low-Resolution Patch, the Middle one is the High-Resolution Patch and the Left most one is the Generated High-Resolution Image.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/output.png#center alt=outputs loading=lazy data-zoomable></div></div></figure></p></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://gautammilan.github.io/post/single-image-super-resolution/&text=Single%20Image%20Super%20Resolution" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://gautammilan.github.io/post/single-image-super-resolution/&t=Single%20Image%20Super%20Resolution" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Single%20Image%20Super%20Resolution&body=https://gautammilan.github.io/post/single-image-super-resolution/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://gautammilan.github.io/post/single-image-super-resolution/&title=Single%20Image%20Super%20Resolution" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Single%20Image%20Super%20Resolution%20https://gautammilan.github.io/post/single-image-super-resolution/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://gautammilan.github.io/post/single-image-super-resolution/&title=Single%20Image%20Super%20Resolution" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> ‚Äî the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/vendor-bundle.min.583938d79e0d9d038283176d43703bc5.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.6b73888494485d3b0874ca2ec83f614f.js type=module></script>
<script src=/en/js/wowchemy.min.31ba171fa2d48135dd9ba17bfb10af4a.js></script></body></html>