<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.4.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Milan Gautam"><meta name=description content="As most of today&rsquo;s world has been digitalized, instead of writing transactions in journals people do it on their computers. In many cases, these transactions have important information about the parties involved in it, so business tries to acquire it by using many techniques."><link rel=alternate hreflang=en-us href=https://gautammilan.github.io/post/entity-extraction/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=/css/wowchemy.246129d782c938a644fe2d653d8a976f.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://gautammilan.github.io/post/entity-extraction/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="MILAN"><meta property="og:url" content="https://gautammilan.github.io/post/entity-extraction/"><meta property="og:title" content="Transactions entity extractor | MILAN"><meta property="og:description" content="As most of today&rsquo;s world has been digitalized, instead of writing transactions in journals people do it on their computers. In many cases, these transactions have important information about the parties involved in it, so business tries to acquire it by using many techniques."><meta property="og:image" content="https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-01-03T21:38:54+05:45"><meta property="article:modified_time" content="2022-01-03T21:38:54+05:45"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://gautammilan.github.io/post/entity-extraction/"},"headline":"Transactions entity extractor","datePublished":"2022-01-03T21:38:54+05:45","dateModified":"2022-01-03T21:38:54+05:45","author":{"@type":"Person","name":"Milan Gautam"},"publisher":{"@type":"Organization","name":"MILAN","logo":{"@type":"ImageObject","url":"https://gautammilan.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"}},"description":"As most of today\u0026rsquo;s world has been digitalized, instead of writing transactions in journals people do it on their computers. In many cases, these transactions have important information about the parties involved in it, so business tries to acquire it by using many techniques."}</script><title>Transactions entity extractor | MILAN</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=ddd9e93cdb794de46453e668fa10a15f><script src=/js/wowchemy-init.min.2539a3058b6536055b0f14f7d5f9df7b.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>MILAN</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>MILAN</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/uploads/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Transactions entity extractor</h1><div class=article-metadata><div><span>Milan Gautam</span></div><span class=article-date>Jan 3, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>9 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/deep-learning/>Deep Learning</a></span></div></div><div class=article-container><div class=article-style><p>As most of today&rsquo;s world has been digitalized, instead of writing transactions in journals people do it on their computers. In many cases, these transactions have important information about the parties involved in it, so business tries to acquire it by using many techniques. In this article, we will look into one such transaction and try to acquire embedded information inside the transaction using the deep learning model.</p><p><figure id=figure-conventional-method><div class="d-flex justify-content-center"><div class=w-100><img src=/images/dataset_image_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Conventional Method</figcaption></figure></p><p>As we can see the transaction contains the store number information, usually people had to go through all of these transactions to get the store number ID. But in today&rsquo;s era, it’s not feasible where thousands of transaction happens every single second using various digital means.</p><br><h2 id=mapping-it-to-machine-learning-problem>Mapping it to machine learning problem</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/model_idea_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div></figure></p><p>So basically we need to build a model which will make the transaction as an input and produce its store number. As we are emphasizing each character and determining whether it is a store number or not. Character LSTM is capable of doing this task, so the ideal choice for solving this problem is a char-LSTM model, so we will be using it.</p><p><figure id=figure-working-of-bidirectional-lstm><div class="d-flex justify-content-center"><div class=w-100><img src=/images/model_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Working of Bidirectional LSTM</figcaption></figure></p><br><p>We will approach this problem as binary classification where the position of store numbers present inside the transaction is labeled as 1 and all the other characters as 0. During inference, we will select all the characters as store numbers that have the prediction of 1. The position of the store number is not rigid and it depends upon the characters appearing on both sides of it. So it is important to look at the transaction from left to right and right to left and then select the store number. Therefore bidirectional LSTM is capable of doing that and we will be using it in this project. Similarly, the size of data that we will be working on is really small only 100 data points for train and we will use other 100 data points for validation, so we will use different training approaches to get the best out of it.</p><p><figure id=figure-dataset><div class="d-flex justify-content-center"><div class=w-100><img src=/images/dataset_distribution_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Dataset</figcaption></figure></p><h2 id=preprocessing>Preprocessing</h2><h3 id=1-1removing-consecutive-whitespaces>1. 1. Removing consecutive Whitespaces</h3><p>We are considering whitespace as a special character, if multiple whitespaces appear in the transaction then they get tokenized using consecutive whitespace tokens, here the model will learn the necessity of adding consecutive whitespaces which is not true. So by removing the consecutive whitespaces altogether we elimate these phenomena. For example</p><p>Before removal: Spark 2001</p><p>After removal: Spark 2001</p><br><h3 id=12-removing-names>1.2 Removing names</h3><p>All the transactions have some kind of name inside them like ANN TAYLOR FACTORY #2202, MCDONALD&rsquo;S F1682, etc. For some models, these names accelerate the training and for some, it works counterproductive. Depending upon the model we will be removing names from the transaction.</p><p>Before removal: Spark 2001</p><p>After removal: 2001</p><br><h3 id=13-tokenization-amd-padding>1.3 Tokenization amd Padding</h3><p>Dictionary is created for all the characters(a,A,c,C&mldr;), non-characters(@,#,%,&mldr;) and digits(1,2,3,4,..). Two special tokens for whitespace character and padding([PAD]) are also included in the dictionary which makes the total size of the vocabulary 87. Finally, all the characters of the transactions are tokenized using the dictionary.</p><p>We have considered that every transaction will have 50 characters, to make sure every transaction has the same length. If the transaction is smaller than this it is padded using padding character [PAD] and if it is larger than 50 characters then it is truncated.</p><br><h3 id=14-label-creating>1.4 Label Creating</h3><p>A list of labels is created for each transaction. If the character is a store number character then it is labeled as 1 elsewhere it is labeled as 0. For transactions such as DOLRTREE 2257 00022574, where the characters forming the store number are located at two places. In such a case the store number characters which are located on the leftmost side are labeled as store number characters and the other one is ignored.</p><p>Transaction: DOLRTREE 2257 00022574</p><p>label :00000000 1111 00000000</p><p>Note: For this example whitespace characters has not be labeled.</p><br><h1 id=models>Models</h1><h2 id=1regular-expression-model>1. Regular expression model</h2><p>By addressing different scenario that occur on training dataset different regular expression operation has been performed on the data. The accuracy for this model is:</p><p>a. Training dataset= 97%</p><p>b. Validation dataset= 87%</p><p>Just by using regular expression the accuracy of validation data is also really good which indicates there is not much difference between the training and validation set. As the possibility of overfitting is really small, we can even train the model for a large number of epochs.</p><br><h2 id=2lstm-model>2. LSTM Model</h2><p>There are two inputs to the model one is the token and the second is the attention mask which is a Boolean matrix indicating which tokens have been padded, it is useful when training and calculating loss so that the loss of the padded token doesn’t get included on our overall loss. Similarly, as the store-number character is dependent on the characters on both sides we will use bidirectional LSTM. Bidirectional LSTM produces two outputs for every single cell, one when viewing the characters from the left and the other from the right, we will merge these outputs by taking an average.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span> 
</span></span><span style=display:flex><span><span style=color:#a6e22e>class</span> <span style=color:#a6e22e>Bi_LSTM</span>(<span style=color:#a6e22e>keras</span>.<span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Layer</span>):
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>def</span> <span style=color:#a6e22e>__init__</span>(<span style=color:#a6e22e>self</span>,<span style=color:#a6e22e>units</span>):
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>super</span>(<span style=color:#a6e22e>Bi_LSTM</span>, <span style=color:#a6e22e>self</span>).<span style=color:#a6e22e>__init__</span>()
</span></span><span style=display:flex><span>        <span style=color:#960050;background-color:#1e0010>#</span><span style=color:#a6e22e>Return_sequences</span>: <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>output</span> <span style=color:#a6e22e>of</span> <span style=color:#a6e22e>every</span> <span style=color:#a6e22e>single</span> <span style=color:#a6e22e>cell</span>
</span></span><span style=display:flex><span>        <span style=color:#960050;background-color:#1e0010>#</span><span style=color:#a6e22e>return_state</span>: <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>hidden</span> <span style=color:#a6e22e>state</span> <span style=color:#a6e22e>of</span> <span style=color:#a6e22e>every</span> <span style=color:#a6e22e>single</span> <span style=color:#a6e22e>cell</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>lstm</span> = <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>LSTM</span>(<span style=color:#a6e22e>units</span>,<span style=color:#a6e22e>return_sequences</span>=<span style=color:#a6e22e>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>bi_lstm</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Bidirectional</span>(<span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>lstm</span>,<span style=color:#a6e22e>merge_mode</span>=<span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>ave</span><span style=color:#960050;background-color:#1e0010>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>def</span> <span style=color:#a6e22e>call</span>(<span style=color:#a6e22e>self</span>, <span style=color:#a6e22e>inputs</span>,<span style=color:#a6e22e>attention_mask</span>):
</span></span><span style=display:flex><span>        <span style=color:#960050;background-color:#1e0010>#</span><span style=color:#a6e22e>It</span> <span style=color:#a6e22e>is</span> <span style=color:#a6e22e>important</span> <span style=color:#a6e22e>to</span> <span style=color:#a6e22e>sending</span> <span style=color:#a6e22e>the</span> <span style=color:#a6e22e>masking</span> <span style=color:#a6e22e>vector</span> <span style=color:#a6e22e>in</span> <span style=color:#a6e22e>order</span> <span style=color:#a6e22e>to</span> <span style=color:#a6e22e>indicate</span> <span style=color:#a6e22e>which</span> <span style=color:#a6e22e>tokens</span> <span style=color:#a6e22e>are</span> <span style=color:#a6e22e>masked</span> <span style=color:#a6e22e>tokens</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>output</span>= <span style=color:#a6e22e>self</span>.<span style=color:#a6e22e>bi_lstm</span>(<span style=color:#a6e22e>inputs</span>,<span style=color:#a6e22e>mask</span>= <span style=color:#a6e22e>attention_mask</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>output</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>def</span> <span style=color:#a6e22e>create</span>():
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>inputs</span>= <span style=color:#a6e22e>tf</span>.<span style=color:#a6e22e>keras</span>.<span style=color:#a6e22e>Input</span>(<span style=color:#a6e22e>shape</span>= (<span style=color:#a6e22e>preprocessor_train</span>.<span style=color:#a6e22e>pad_len</span>),<span style=color:#a6e22e>dtype</span>= <span style=color:#a6e22e>tf</span>.<span style=color:#66d9ef>float32</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>att_mask</span>= <span style=color:#a6e22e>tf</span>.<span style=color:#a6e22e>keras</span>.<span style=color:#a6e22e>Input</span>(<span style=color:#a6e22e>shape</span>= (<span style=color:#a6e22e>preprocessor_train</span>.<span style=color:#a6e22e>pad_len</span>),<span style=color:#a6e22e>dtype</span>= <span style=color:#a6e22e>tf</span>.<span style=color:#66d9ef>bool</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Embedding</span>(len(<span style=color:#a6e22e>dictionary</span>),<span style=color:#ae81ff>50</span>)(<span style=color:#a6e22e>inputs</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#960050;background-color:#1e0010>#</span><span style=color:#a6e22e>Here</span> <span style=color:#a6e22e>we</span> <span style=color:#a6e22e>will</span> <span style=color:#a6e22e>be</span> <span style=color:#a6e22e>using</span> <span style=color:#a6e22e>two</span> <span style=color:#a6e22e>LSTM</span> <span style=color:#a6e22e>layers</span> <span style=color:#a6e22e>each</span> <span style=color:#a6e22e>followed</span> <span style=color:#a6e22e>by</span> <span style=color:#a6e22e>an</span> <span style=color:#a6e22e>dropout</span> <span style=color:#a6e22e>layer</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>Bi_LSTM</span>(<span style=color:#ae81ff>126</span>)(<span style=color:#a6e22e>x</span>, <span style=color:#a6e22e>attention_mask</span>= <span style=color:#a6e22e>att_mask</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Dropout</span>(<span style=color:#ae81ff>0.3</span>)(<span style=color:#a6e22e>x</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>Bi_LSTM</span>(<span style=color:#ae81ff>65</span>)(<span style=color:#a6e22e>x</span>, <span style=color:#a6e22e>attention_mask</span>= <span style=color:#a6e22e>att_mask</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Dropout</span>(<span style=color:#ae81ff>0.3</span>)(<span style=color:#a6e22e>x</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>TimeDistributed</span>(<span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Dense</span>(<span style=color:#ae81ff>32</span>))(<span style=color:#a6e22e>x</span>,<span style=color:#a6e22e>mask</span>= <span style=color:#a6e22e>att_mask</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>x</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Dropout</span>(<span style=color:#ae81ff>0.3</span>)(<span style=color:#a6e22e>x</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>output</span>= <span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>TimeDistributed</span>(<span style=color:#a6e22e>layers</span>.<span style=color:#a6e22e>Dense</span>(<span style=color:#ae81ff>1</span>,<span style=color:#a6e22e>activation</span>=<span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>sigmoid</span><span style=color:#960050;background-color:#1e0010>&#39;</span>))(<span style=color:#a6e22e>x</span>,<span style=color:#a6e22e>mask</span>= <span style=color:#a6e22e>att_mask</span>)
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>model</span>= <span style=color:#a6e22e>tf</span>.<span style=color:#a6e22e>keras</span>.<span style=color:#a6e22e>Model</span>(<span style=color:#a6e22e>inputs</span>= [<span style=color:#a6e22e>inputs</span>,<span style=color:#a6e22e>att_mask</span>],<span style=color:#a6e22e>outputs</span>= <span style=color:#a6e22e>output</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>model</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>model</span>= <span style=color:#a6e22e>create</span>()
</span></span><span style=display:flex><span><span style=color:#a6e22e>model</span>.<span style=color:#a6e22e>summary</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> 
</span></span></code></pre></div><br><p><figure id=figure-architecture><div class="d-flex justify-content-center"><div class=w-100><img src=/images/architecture_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Architecture</figcaption></figure></p><p>In this architecture there are two bidirectional LSTM layers each followed by a dropout layer with a probability of 0.3, the output of the second layer is provided as input for the dense layer which is timely distributed.</p><br><h2 id=evaluation-on-different-loss-function>Evaluation on different loss function</h2><h4 id=11-simple-bce-loss>1.1 Simple BCE loss</h4><p>Binary cross-entropy loss is the average logistic loss on every prediction. Lets look at the output generated by this loss on validation dataset:</p><p><figure id=figure-output><div class="d-flex justify-content-center"><div class=w-100><img src=/images/simple_BCE_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Output</figcaption></figure></p><p>Here the input is labeled as 1 when the output prediction is more than 0.5. Now lets look at the accuracy of train and test set on different value of thresholds:</p><p><figure id=figure-accuracy-vs-threshold><div class="d-flex justify-content-center"><div class=w-100><img src=/images/simple_BCE_plot_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Accuracy vs Threshold</figcaption></figure></p><p>At threshold 0.6 the train and test accuracy is 80% and 60% respectively.</p><p>Assume you have a &ldquo;Atalanta 23&rdquo; transaction with a store number of 23. It has 11 characters, including whitespace; we&rsquo;re computing the value of loss for each of these characters, and we&rsquo;re assuming the average loss for this transaction is 0.5. Because there are more non-store number characters than store number characters here, the average loss value shifts toward non-store number characters. As a result, when the model performs incorrect classification on store number characters, its contribution to the average loss remains small, resulting in a small penalization of models, so the model does not focus on predicting the store number character, but only on predicting non-store-number characters.</p><br><h4 id=12-weighted-bce-loss>1.2 Weighted BCE loss</h4><p>We will generate a weight value for both labels 0 and 1 so that more weights are assigned to the less often label, and these weights are multiplied to the loss value of that label, to remove the biases of average loss value toward the non-store number character. As a result, assigning more weight to the less common label 1 helps to ensure that both labels receive equal attention. The output it produced at threshold 0.5, as well as accuracies at other thresholds:</p><p><figure id=figure-output><div class="d-flex justify-content-center"><div class=w-100><img src=/images/WBCE_output_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Output</figcaption></figure></p><br><p><figure id=figure-accuracy-vs-threshold><div class="d-flex justify-content-center"><div class=w-100><img src=/images/WBCE_output_plot_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Accuracy vs Threshold</figcaption></figure></p><p>The accuracy is poorer than compared to simple BCE loss, it has the maximum test accuracy of 35% at threshold 0.6.</p><br><h4 id=13-simple-bce-loss-after-removing-the-names>1.3 Simple BCE loss after removing the names</h4><p>Taking the names out of the transaction is another technique to eliminate the imbalance. Almost the majority of the names in the transaction are not store number characters, raising the number of negative labels. We can achieve some equilibrium between label non-store number characters and store number characters by deleting names.</p><p><figure id=figure-output><div class="d-flex justify-content-center"><div class=w-100><img src=/images/RE_output_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Output</figcaption></figure></p><br><p><figure id=figure-accuracy-vs-threshold><div class="d-flex justify-content-center"><div class=w-100><img src=/images/RE_plot_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Accuracy vs Threshold</figcaption></figure></p><p>It worked better than weighted BCE loss producing a test accuracy of 55% at a threshold of 0.6. But still, simple BCE loss outperforms it by 5% accuracy on test data.</p><br><h5 id=14-dice-loss>1.4 Dice Loss</h5><p><figure id=figure-dice-loss-working><div class="d-flex justify-content-center"><div class=w-100><img src=/images/DL_work_entity_extractor.png#center alt="(Conventional Method)" loading=lazy data-zoomable></div></div><figcaption>Dice Loss Working</figcaption></figure></p><p>We were attempting to calculate a loss value for each character in the logistic loss, however, the characters will be unaware of their relationships. Let&rsquo;s say there are four store number characters in a transaction, three of which are labeled 1, and one is labeled 0. Although the logistic loss function performed admirably in this case, the actual output is inaccurate. Because the logistic loss function is unaware of the link between the characters&rsquo; loss values, it will simply attempt to minimize the average loss, which is contradictory to the goal. At the same time, it just considers character level loss and ignores the expected store number as a whole. The best loss function for this case appears to be Dice loss which increases the overlap between the predicted sequence of output to the actual output.</p><p><figure id=figure-output><div class="d-flex justify-content-center"><div class=w-100><img src=/images/dice_loss_WNR_output_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Output</figcaption></figure></p><p>As we can see the model is predicting unwanted sequence of characters, due to which it&rsquo;s accuracy has been reduced drastically. The test accuracy is only 30%.</p><p><figure id=figure-accuracy-vs-threshold><div class="d-flex justify-content-center"><div class=w-100><img src=/images/DL_WNR_output_plot_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Accuracy vs Threshold</figcaption></figure></p><br><h4 id=15-dice-loss-loss-after-removing-the-names>1.5 Dice Loss loss after removing the names</h4><p>The discrepancy between the transaction and the store number is relatively little when the names are removed from the transaction. Dice loss works by increasing the intersection between the actual and anticipated sequences of store number characters. By removing names from the transaction, the length of the transaction is lowered, which minimizes the number of undesirable sequence combinations that we want our dice loss to avoid.</p><p><figure id=figure-output><div class="d-flex justify-content-center"><div class=w-100><img src=/images/DL_NR_output_plot_entity_extractor.png#center alt="Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Output</figcaption></figure></p><p>We attained the highest accuracy of 72% on the test dataset by doing so, making it the best strategy for training this dataset.</p><p><figure id=figure-accuracy-vs-threshold><div class="d-flex justify-content-center"><div class=w-100><img src=/images/plot_entity_extractor.png#center alt="Alt Conventional Method" loading=lazy data-zoomable></div></div><figcaption>Accuracy vs Threshold</figcaption></figure></p><br><h1 id=traning-parameter>Traning Parameter</h1><p>The BCE loss was trained for 1000 epochs with a learning rate of 1e-5. Similarly, with an initial learning rate of 1e-4, the dice loss was trained for 200 epochs. The Adam optimizer was employed, with a polynomial learning rate decay and the first 100 steps of the models operating as warn steps.</p></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://gautammilan.github.io/post/entity-extraction/&text=Transactions%20entity%20extractor" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://gautammilan.github.io/post/entity-extraction/&t=Transactions%20entity%20extractor" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Transactions%20entity%20extractor&body=https://gautammilan.github.io/post/entity-extraction/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://gautammilan.github.io/post/entity-extraction/&title=Transactions%20entity%20extractor" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Transactions%20entity%20extractor%20https://gautammilan.github.io/post/entity-extraction/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://gautammilan.github.io/post/entity-extraction/&title=Transactions%20entity%20extractor" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/vendor-bundle.min.9592335d574f7a97010f99b90ad0f310.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.6b73888494485d3b0874ca2ec83f614f.js type=module></script>
<script src=/en/js/wowchemy.min.e14897cafececa7181d21d62540ef117.js></script></body></html>